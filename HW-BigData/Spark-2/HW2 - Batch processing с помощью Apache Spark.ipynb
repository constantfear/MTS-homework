{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2b06d19-8478-4df4-bb2d-99dfa4f07aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e1909f1-a5fa-4393-8213-a0c8bca6525d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "com.amazonaws#aws-java-sdk-pom added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-c1e405e5-032d-4d51-bee7-62d9653bc748;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.2 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.1026 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "\tfound com.amazonaws#aws-java-sdk-pom;1.12.365 in central\n",
      ":: resolution report :: resolve 117ms :: artifacts dl 4ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.1026 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-pom;1.12.365 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.2 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-c1e405e5-032d-4d51-bee7-62d9653bc748\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/2ms)\n",
      "24/06/20 09:37:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "ACCESS_KEY = \"<YOUR_ACCESS_KEY>\"\n",
    "SECRET_KEY = \"<YOUR_SECRET_KEY>\"\n",
    "MINIO_URL = \"http://minio:9000\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .appName(\"HW2\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", False) \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", -1) \\\n",
    "    .config(\"spark.sql.sources.bucketing.enabled\", True) \\\n",
    "    .config(\"spark.executor.memory\", \"450M\") \\\n",
    "    .config(\"spark.driver.memory\", \"450M\") \\\n",
    "    .config('spark.jars.packages', \n",
    "        \"org.apache.hadoop:hadoop-aws:3.3.2,com.amazonaws:aws-java-sdk-pom:1.12.365,org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\"\n",
    "    ) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider') \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", ACCESS_KEY) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", SECRET_KEY) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", MINIO_URL) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb74cf6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02f9ece4-83c7-486a-84b6-192fca40d533",
   "metadata": {},
   "source": [
    "# Задание 1\n",
    "\n",
    "## Входные данные \n",
    "- Файл с данными по оттоку телеком оператора в США (churn.csv)\n",
    "- Справочник с названиями штатов (state.json)\n",
    "- Справочник с численностью населения территорий (определяется полем area code) внутри штатов (state.json)\n",
    "- Террия с численностью населения меньше 10_000 считается **мелкой**\n",
    "\n",
    "## Что нужно сделать\n",
    "1. Посчитать количество отточных и неотточных абонентов (поле churn), исключив **мелкие** территории\n",
    "2. Отчет должен быть выполнен в разрезе **каждого штата** с его полным наименованием\n",
    "3. Описать возникающие узкие места при выполнении данной операции\n",
    "4. Применить один из способов оптимизации для ускорения выполнения запроса (при допущении, что справочник численности населения **сильно меньше** основных данных)\n",
    "5. Если существует еще какой-то способ, применить также и его отдельно от п.4 (при допущении, что справочник численности населения **сопоставим по размеру** с основными данными)\n",
    "6. Кратко описать реализованные способы и в чем их практическая польза\n",
    "\n",
    "- P.S. Одним из выбранных способов должен быть Bucket specific join\n",
    "- P.P.S. При обосновании предлагаем прикладывать запуска команды df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fd39bc-4dbf-4deb-bab3-2a3ef7ee67ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b58da6b-e2ff-4cd6-9995-c4d35e5c2745",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/20 09:37:45 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "churn_df = spark.read.option(\"header\", True).csv(\"s3a://input/data/churn.csv\")\n",
    "state_dict = spark.read.json(\"s3a://input/data/state.json\")\n",
    "pop_dict = spark.read.json(\"s3a://input/data/population.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18763c6",
   "metadata": {},
   "source": [
    "### Решение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7bd521c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваше решение в ячейках ниже ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaee4ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14b64c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Здесь необходимо вывести результат:\n",
    "# result.explain()\n",
    "# result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58ca16c",
   "metadata": {},
   "source": [
    "### Оптимизация 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4663dabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваше решение в ячейках ниже ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8db731b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea922ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Здесь необходимо вывести результат:\n",
    "# result.explain()\n",
    "# result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac80fd0",
   "metadata": {},
   "source": [
    "### Оптимизация 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c18f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваше решение в ячейках ниже ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a71375",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d446130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Здесь необходимо вывести результат:\n",
    "# result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dc61ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40f06364",
   "metadata": {},
   "source": [
    "# Задание 2\n",
    "\n",
    "## Входные данные \n",
    "\n",
    "*skew_transactions.csv* - информация о длительности просомтра контента пользователям\n",
    "колонки:\n",
    "1. user_uid — уникальный идентификатор пользователя\n",
    "2. element_uid — уникальный идентификатор контента\n",
    "3. watched_time — время просмотра в секундах\n",
    "\n",
    "*catalogue.json* - каталог с описанием контента и метаинформации по нему\n",
    "колонки:\n",
    "1. type — тип элемента\n",
    "2. duration — длительность в минутах (средняя длительность эпизода в случае с сериалами и многосерийными фильмами), округлённая до десятков\n",
    "3. attributes — анонимизированные атрибуты данного элемента\n",
    "4. availability — доступные права на элемент(subscription, purchase, rent)\n",
    "5. feature_1 — анонимизированная вещественная переменная\n",
    "6. feature_2 — анонимизированная вещественная переменная\n",
    "7. feature_3 — анонимизированная порядковая переменная\n",
    "8. feature_4 — анонимизированная вещественная переменная\n",
    "9. feature_5 — анонимизированная вещественная переменная\n",
    "\n",
    "## Что нужно сделать\n",
    "1. Выполните join основных данных со справочником используя DataFrame API (по колонке id для контента - `element_uid`)\n",
    "2. Описать проблему в датасетах с точки зрения обработки Spark\n",
    "3. Решить задачу любым способом\n",
    "4. Решить задачу с помощью salt-join подхода\n",
    "\n",
    "P.S. Как вы можете заметить при просмотре данных по пользователями, нужный нам ключ для операции будет перекошен (90% строк представлены на фильм, очень популярный среди смотревших) - это нужно доказать в рамках п.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0740055d",
   "metadata": {},
   "source": [
    "### Решение "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2e8c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваше решение в ячейках ниже ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde95359",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df30c61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Здесь необходимо вывести результат:\n",
    "# result.explain()\n",
    "# result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108c868f",
   "metadata": {},
   "source": [
    "### Решение с оптимизацией"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6445a89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваше решение в ячейках ниже ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c875e042",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a6cecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Здесь необходимо вывести результат:\n",
    "# result.explain()\n",
    "# result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238efd2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01898809",
   "metadata": {},
   "source": [
    "# Задание 3\n",
    "\n",
    "## Входные данные \n",
    "\n",
    "*cut_transactions.csv*  — информация о длительности просомтра контента пользователям\n",
    "\n",
    "Описание фичей в cut_transactions.csv: \n",
    "1. user_uid — уникальный идентификатор пользователя\n",
    "2.  element_uid — уникальный идентификатор контента\n",
    "3.  watched_time — время просмотра в секундах\n",
    "\n",
    "*cut_ratings.csv*  — информация об оценках, поставленных пользователями\n",
    "\n",
    "Описание фичей в cut_ratings.csv: \n",
    "1. user_uid — уникальный идентификатор пользователя \n",
    "2. element_uid — уникальный идентификатор контента \n",
    "3. rating — поставленный пользователем рейтинг\n",
    "\n",
    "*ids.csv*  — выборка пользователей\n",
    "Описание фичей в ids.csv: \n",
    "1. user_uid — уникальный идентификатор пользователя \n",
    "\n",
    "\n",
    "## Что нужно сделать\n",
    "Для каждого пользователя из выборки посчитать:\n",
    "1. Максимальное и минимальное время просмотра фильмов с оценками 8, 9 и 10 \n",
    "2. Название фичи должно быть в формате feat_агрегирующая_функция_watched_time_rating_оценка. \n",
    "3. Если у пользователь не ставил оценки 8, 9 и 10 то значение фичей должно быть null\n",
    "4. Описать принятые при разработки кода решения и возможные оптимизации\n",
    "\n",
    "P.S. На каждом этапе обработки должно быть должны агрегироваться минимально возможные объемы данных (сокращаем затраты на shuflle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb99364",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0963ed4",
   "metadata": {},
   "source": [
    "### Решение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dac489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваше решение в ячейках ниже ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fc85fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd40363a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Здесь необходимо вывести результат:\n",
    "# result.explain()\n",
    "# result.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
